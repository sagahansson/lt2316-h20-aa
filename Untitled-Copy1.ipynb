{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "from nltk.tokenize import WhitespaceTokenizer as wst\n",
    "import re\n",
    "#from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(rootdir):\n",
    "    # fetches a list of absolute paths, given a dir with xml files\n",
    "    # BEHÖVS FÖR OPEN_XMLS\n",
    "    file_paths = []\n",
    "\n",
    "    for folder, _, files in os.walk(rootdir):\n",
    "        for filename in files:\n",
    "            if filename.endswith('xml'):\n",
    "                file_paths.append(os.path.abspath(os.path.join(folder, filename)))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = get_paths(\"DDICorpus/Train/DrugBank\")\n",
    "allFiles.extend(get_paths(\"DDICorpus/Train/MedLine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"string. With. Punctuation? don't 'hell0o cat's'\"\n",
    "def string_to_span(s):\n",
    "    # creates a tokenized version and a span version of a string\n",
    "    # BEHÖVS FÖR OPEN_XMLS\n",
    "    s = (re.sub(r\"[^A-Za-z\\s]\",'',s)).lower() # removes all non-alphanumerical characters\n",
    "    tokenized = wst().tokenize(s)\n",
    "    span = list(wst().span_tokenize(s)) # gets the pythonic span i e (start, stop_but_not_including)\n",
    "    new_span = []\n",
    "    for tpl in span:\n",
    "        new_span.append((tpl[0], (tpl[1]-1))) # to get non-pythonic span i e (start,last_char)\n",
    "    return new_span, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_xmls(fileList):\n",
    "    \n",
    "    vocab = []\n",
    "    data_df_list = [] \n",
    "    ner_df_list = []\n",
    "    ent_types = []\n",
    "    all_words = []\n",
    "    for file in fileList:\n",
    "        print(\"   -------------- this is one file ---------------------\")\n",
    "        print(f\"filename: {file}\")\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        #tag = root.tag\n",
    "        for sentence in root:\n",
    "            print(\"--------------------- this is one sentence ---------------------\")\n",
    "            print(f\"filename is still: {file}\")\n",
    "            sent_id = sentence.attrib['id']\n",
    "            sent_txt= sentence.attrib['text']\n",
    "            \n",
    "            print(\"sentence id :\", sent_id)\n",
    "            print(\"sentence text:\", sent_txt)\n",
    "            split = sent_txt.split(' ') # do i need this?\n",
    "            char_ids, tokenized = string_to_span(sent_txt)\n",
    "            print(tokenized)\n",
    "            #print(f\"char ids: {char_ids}\")\n",
    "            #print(f\"tokenized: {tokenized}\")\n",
    "            all_words.extend(tokenized)\n",
    "            unique_w = list(set(tokenized))\n",
    "            vocab.extend(unique_w)\n",
    "            for i, word in enumerate(tokenized): # make this into a funciton?\n",
    "                # make a tuple for each word so that the data_df is of the right height \n",
    "                # containing sentence id, actual word since token_id isn't available yet, char_start_id, char_end_id\n",
    "                word_tpl = (sent_id, word, char_ids[i][0], char_ids[i][1])# one row in data_df \n",
    "                \n",
    "                data_df_list.append(word_tpl)\n",
    "            for entity in sentence:\n",
    "                if entity.tag == 'entity':\n",
    "                    ent_txt = (entity.attrib['text']).lower()\n",
    "                    ent_type = (entity.attrib['type']).lower()\n",
    "                    #print(entity.attrib['charOffset'])\n",
    "                    #ent_start_id, ent_end_id = (entity.attrib['charOffset']).split('-')\n",
    "                    print(\"type: \", ent_type)\n",
    "                    print(\"entity text ie the actual word:\", ent_txt)\n",
    "                    \n",
    "                    #print(\"entity charOffset ie start and end\", ent_start_id)\n",
    "                    #print(\"entity charOffset ie start and end\", ent_end_id)\n",
    "                #write csv line with all this info and ie sentence id, char start & end\n",
    "                # also simultaneously gather words for vocab : ) then \n",
    "                    ent_types.append(ent_type)\n",
    "                    entity_tpl = (sent_id, ent_type) # ent_start_id, ent_end_id\n",
    "                    \n",
    "                    ner_df_list.append(entity_tpl)\n",
    "    \n",
    "    vocab = list(sorted(set(vocab)))\n",
    "    return vocab, data_df_list, ner_df_list, ent_types, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data_df_list, ner_df_list, entities, all_words= open_xmls(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data_df_list, columns=['sent id', 'token', 'char start id', 'char end id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count['drug'] + count['drug_n'] + count['group'] + count['brand']) /(len(all_words)) # obalans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar hela vocabet från open_xmls\n",
    "def word2int(vocabList):\n",
    "    #w2i = {} # dictionary mapping words to integers\n",
    "    #for i, w in enumerate(sorted((vocabList))):\n",
    "    #    w2i[w] = i # because we want to be able to get integers by giving word as a key\n",
    "    #return w2i\n",
    "    \n",
    "    return {w:i for i,w in enumerate(sorted(vocabList))} # FASTER\n",
    "w2i = word2int(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efter data_df_list är en dataframe och efter vocab har blivit w2i\n",
    "# tar data_df-kolumnen för tokens\n",
    "def get_token_ids(tokensList, vocab2idDict):\n",
    "    # fetches token id from vocab2id dict\n",
    "    #token_ids = []\n",
    "    #for w in tokensList:\n",
    "    #    print(w)\n",
    "    #    token_ids.append(vocab2idDict[w])\n",
    "    #return token_ids\n",
    "    return [vocab2idDict[w] for w in tokensList] # SNABBARE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "w2i = word2int(vocab)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf['token id'] = token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pddf.drop(columns=['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idd['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus = \"Drug interaction studies with SUSTIVA and these imidazole and triazole antifungals have not been conducted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['cat', 'dog', 'hamster', 'monkey', 'banana']\n",
    "b = ['cat', 'hamster', 'apple', 'pear', 'banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(a) ^ set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "init = \"a = ['cat', 'dog', 'hamster', 'monkey', 'banana']; b = ['cat', 'hamster', 'apple', 'pear', 'banana']\"\n",
    "print(timeit.timeit('list(set(a) - set(b))', init, number = 100000))\n",
    "print(timeit.timeit('s = set(b);[x for x in a if x not in s]', init, number = 100000))\n",
    "print(timeit.timeit('set(a) ^ set(b)', init, number = 100000))\n",
    "print(timeit.timeit('[item for item in a if item not in b]', init, number = 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('eggs.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['Spam'] * 5 + ['Baked Beans'])\n",
    "    spamwriter.writerow(['Spam', 'Lovely Spam', 'Wonderful Spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"string With. Punctuation: don't HElL0 c4T5.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (re.sub(r\"[^A-Za-z\\s]\",'',abc)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zxc = string_to_span(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfObj = pd.DataFrame(zxc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = \"The concurrent use of Robinul Injection with other anticholinergics or medications with anticholinergic activity, such as phenothiazines, antiparkinson drugs, or tricyclic antidepressants, may intensify the antimuscarinic effects and may result in an increase in anticholinergic side effects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(z): \n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T NEED THIS \n",
    "# list(spans_to_relative(wst().span_tokenize(s))) # how long the words are"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
