{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "from nltk.tokenize import WhitespaceTokenizer as wst\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "#from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(rootdir):\n",
    "    # fetches a list of absolute paths, given a dir with xml files\n",
    "    # BEHÖVS FÖR OPEN_XMLS\n",
    "    file_paths = []\n",
    "\n",
    "    for folder, _, files in os.walk(rootdir):\n",
    "        for filename in files:\n",
    "            if filename.endswith('xml'):\n",
    "                file_paths.append(os.path.abspath(os.path.join(folder, filename)))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = get_paths(\"../DDICorpus/\")\n",
    "#allFiles.extend(get_paths(\"../DDICorpus/Test\")) # ska vi ha båda testmapparna???\n",
    "#allFiles.extend(get_paths(\"../DDICorpus/Test/Test for DrugNER task\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"string. With. Punctuation? don't 'hell0o cat's' micro-organisms 'chat'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_span(s):\n",
    "    # creates a tokenized version and a span version of a string\n",
    "    # BEHÖVS FÖR OPEN_XMLS\n",
    "    #s = (re.sub(r\"[^A-Za-z\\s]\",'',s)).lower() # removes all non-alphanumerical characters LOL gör inte det\n",
    "    punctuation = \"-,.?!:;\"\n",
    "    tokenizer = RegexpTokenizer(\"\\s|:|;\", gaps=True)\n",
    "    tokenized = tokenizer.tokenize(s.lower())\n",
    "    [word.strip(punctuation) if word[-1] in punctuation else word for word in tokenized]\n",
    "    #list(tokenized.span_tokenize(s))\n",
    "    #tokenizer = RegexpTokenizer(\"[\\w'-]+|[^\\w\\s]+\") # tokenizes words and punctuation except hyphens in compound words and apostrophes\n",
    "    #tokenized = tokenizer.tokenize(s.lower())\n",
    "    span = list(tokenizer.span_tokenize(s)) # gets the pythonic span i e (start, stop_but_not_including)\n",
    "    new_span = []\n",
    "    for tpl in span:\n",
    "        new_span.append((tpl[0], (tpl[1]-1))) # to get non-pythonic span i e (start,last_char)\n",
    "    return new_span, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_xmls(fileList):\n",
    "    \n",
    "    vocab = []\n",
    "    data_df_list = [] \n",
    "    ner_df_list = []\n",
    "    ent2id = {\n",
    "        'drug'   : 0,\n",
    "        'drug_n' : 1,\n",
    "        'group'  : 2, \n",
    "        'brand'  : 3\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for file in fileList:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        for sentence in root:\n",
    "            sent_id = sentence.attrib['id']\n",
    "            #if sent_id == \"DDI-DrugBank.d64.s87\":\n",
    "            #    print(f\"filename: {file}\")\n",
    "            sent_txt= sentence.attrib['text']\n",
    "            char_ids, tokenized = string_to_span(sent_txt)\n",
    "            unique_w = list(set(tokenized))\n",
    "            vocab.extend(unique_w)\n",
    "            for i, word in enumerate(tokenized): # creating data_df_list\n",
    "                if 'test' in file.lower():\n",
    "                    split = 'test'\n",
    "                else:\n",
    "                    split = 'train/dev'\n",
    "                word_tpl = (sent_id, word, int(char_ids[i][0]), int(char_ids[i][1]), split) # one row in data_df \n",
    "                data_df_list.append(word_tpl)\n",
    "                \n",
    "            for entity in sentence: # creating the ner_df_list\n",
    "                if entity.tag == 'entity':\n",
    "                    ent_txt = (entity.attrib['text']).lower()\n",
    "                    ent_type = (entity.attrib['type']).lower()\n",
    "                    ent_type = ent2id[ent_type]\n",
    "                    char_offset = entity.attrib['charOffset']\n",
    "                    char_span = (re.sub(r\"[^0-9]+\",' ', char_offset)).split(' ')\n",
    "                    \n",
    "                    if len(char_span) > 2:\n",
    "                        char_pairs = (list(zip(char_span[::2], char_span[1::2])))\n",
    "                        for pair in char_pairs:\n",
    "                            entity_tpl = (sent_id, ent_type, int(pair[0]), int(pair[1]))\n",
    "                            ner_df_list.append(entity_tpl)\n",
    "                    else:\n",
    "                        ent_start_id, ent_end_id = char_span\n",
    "                        ent_txt_one = ent_txt    \n",
    "                        \n",
    "                        entity_tpl = (sent_id, ent_type, int(ent_start_id), int(ent_end_id))\n",
    "                        \n",
    "                        ner_df_list.append(entity_tpl)\n",
    "                        \n",
    "    vocab = list(sorted(set(vocab)))\n",
    "    return vocab, data_df_list, ner_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data_df_list, ner_df_list  = open_xmls(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar hela vocabet från open_xmls\n",
    "def word2int(vocabList):\n",
    "    #w2i = {} # dictionary mapping words to integers\n",
    "    #for i, w in enumerate(sorted((vocabList))):\n",
    "    #    w2i[w] = i # because we want to be able to get integers by giving word as a key\n",
    "    #return w2i\n",
    "    \n",
    "    return {w:i for i,w in enumerate(sorted(vocabList))} # FASTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efter data_df_list är en dataframe och efter vocab har blivit w2i\n",
    "# tar data_df-kolumnen för tokens och dicten från w2i\n",
    "def get_token_ids(tokensList, vocab2idDict):\n",
    "    # fetches token id from vocab2id dict\n",
    "    #token_ids = []\n",
    "    #for w in tokensList:\n",
    "    #    print(w)\n",
    "    #    token_ids.append(vocab2idDict[w])\n",
    "    #return token_ids\n",
    "    return [vocab2idDict[w] for w in tokensList] # SNABBARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data_df_list, columns=['sentence_id', 'token', 'char_start_id', 'char_end_id', 'split'])\n",
    "ner_df = pd.DataFrame(ner_df_list, columns=['sentence_id', 'ner_id', 'char_start_id', 'char_end_id']) # ner_id = entity type\n",
    "w2i = word2int(vocab)\n",
    "token_ids = get_token_ids(data_df['token'], w2i)\n",
    "data_df.insert(1, 'token_id', token_ids)\n",
    "#data_df['token id'] = token_ids\n",
    "data_df = data_df.drop(columns=['token'])\n",
    "test_df = data_df.loc[data_df.split == 'test']\n",
    "traindev_df = data_df.loc[data_df.split != 'test']\n",
    "dev_len = len(test_df) # size of dev/val set is decided from how big the test set is\n",
    "train_len = len(traindev_df) - dev_len\n",
    "traindev_df.drop(columns=['split'])\n",
    "train_dev = ['train'] * train_len\n",
    "dev = ['dev'] * dev_len\n",
    "train_dev.extend(dev)\n",
    "shuffle(train_dev)\n",
    "#traindev_df['split'] = shuffled\n",
    "pd.options.mode.chained_assignment = None\n",
    "traindev_df.loc[:, 'split'] = train_dev\n",
    "data_df = (traindev_df.append(test_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = Counter(list(data_df.sentence_id)) # counting occurences of sentence_id in data df = len of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample_length = max(sent_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get sentences out of data_dt\n",
    "sentences= []\n",
    "for x in sent_dict.keys():\n",
    "    sentences.append(list(data_df.loc[data_df['sentence_id'] == x, 'token_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = Counter(list(data_df.sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "max(sent_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sent_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no = (traindev_df.sentence_id.nunique()) - (test_df.sentence_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no = data_df.sentence_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no = test_df.sentence_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count['drug'] + count['drug_n'] + count['group'] + count['brand']) /(len(all_words)) # obalans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ddata_df.sort_values(by=['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idd['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.00021696090698242188\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus = \"Drug interaction studies with SUSTIVA and these imidazole and triazole antifungals have not been conducted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['cat', 'dog', 'hamster', 'monkey', 'banana']\n",
    "b = ['cat', 'hamster', 'apple', 'pear', 'banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(a) ^ set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "init = \"a = ['cat', 'dog', 'hamster', 'monkey', 'banana']; b = ['cat', 'hamster', 'apple', 'pear', 'banana']\"\n",
    "print(timeit.timeit('list(set(a) - set(b))', init, number = 100000))\n",
    "print(timeit.timeit('s = set(b);[x for x in a if x not in s]', init, number = 100000))\n",
    "print(timeit.timeit('set(a) ^ set(b)', init, number = 100000))\n",
    "print(timeit.timeit('[item for item in a if item not in b]', init, number = 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"string With. Punctuation: don't HElL0 c4T5.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (re.sub(r\"[^0-9]+\",' ',nums)).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kates = \"I shouldn\\'t really but, okay I will. I love 'Rocky' and calcium-rich foods -- bananas for example. 'jajjaj.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = (re.sub(r\"\\w+|[^\\w\\s]+\",' ',kates)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[0-9]+\")\n",
    "tokenizer.tokenize(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = \"11-5123;502-112;838-999\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w'\\w]+|[^\\w\\s]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.tokenize(kates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = ['85', '92', '103', '111', '146', '153']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toktok = \"thiazide diuretics paraply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(tok1[1]) - int(tok1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toktok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lol:\n",
    "    print(int(item[1])- int(item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_w = [int(tpl[1])-int(tpl[0]) for tpl in lol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltoktok = list(toktok)\n",
    "def get_word_span(charOffsetList, entTxt, sentId, entType):\n",
    "    \n",
    "    ners = []\n",
    "    span_pairs = list(zip(charOffsetList[::2], charOffsetList[1::2]))\n",
    "    ent_txt = list(entTxt)\n",
    "    w_len = [int(tpl[1])-int(tpl[0]) for tpl in span_pairs] # len of each word in entity\n",
    "    \n",
    "    for i, n in enumerate(w_len):\n",
    "\n",
    "        ners.append((''.join((ent_txt[:n+1])), span_pairs[i][0], span_pairs[i][1]))\n",
    "        #print(lol[i])\n",
    "        #print(ltoktok)\n",
    "        del ent_txt[:n+2]\n",
    "    return ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_word_span(tok1, toktok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(get_word_span(tok1, toktok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lol = list(zip(tok1[::2], tok1[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(kates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zxc = string_to_span(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfObj = pd.DataFrame(zxc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = \"The concurrent use of Robinul Injection with other anticholinergics or medications with anticholinergic activity, such as phenothiazines, antiparkinson drugs, or tricyclic antidepressants, may intensify the antimuscarinic effects and may result in an increase in anticholinergic side effects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(z): \n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T NEED THIS \n",
    "# list(spans_to_relative(wst().span_tokenize(s))) # how long the words are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringy = \"string. With. Punctuation? don't 'hell0o cat's micro-organisms 'chat' 1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine punctu:aaaaation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w,'-]+\") # tokenizes words and punctuation except hyphens in compound words and apostrophes\n",
    "tokenized = tokenizer.tokenize(stringy.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenizer.span_tokenize(stringy.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"-,.?!:;\"\n",
    "tok = RegexpTokenizer(\"\\s|:|;\", gaps=True)\n",
    "x = tok.tokenize(stringy.lower())\n",
    "[string.strip(punctuation) if string[-1] in punctuation else string for string in x]\n",
    "list(tok.span_tokenize(stringy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wst().span_tokenize(stringy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df['token'] != 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "max(sent_dict.values())\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.004487276077270508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcv = \"Other compounds that are substrates of CYP3A4 may have decreased plasma concentrations when coadministered with SUSTIVA (efavirenz).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcv[112:119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokenizer = RegexpTokenizer(\"[\\w'-]+|[^\\w\\s]+\") # tokenizes words and punctuation except hyphens in compound words and apostrophes\n",
    "    tokenized = tokenizer.tokenize(xcv.lower())\n",
    "    span = list(tokenizer.span_tokenize(xcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
